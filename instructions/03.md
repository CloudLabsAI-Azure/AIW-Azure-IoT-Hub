# Exercise 3: Machine Learning in IoT

## Scenario

Insights generated so far require manual involvement to detect anomalies. We need to make use of automated techniques to detect anomalies.
Upon anomaly detection, the issue needs to be escalated as soon as possible.

## Overview

In this exercise, you will use automated in-built Machine Learning models to analyze the data from the Turbine devices and detect anomalies instead of detecting anomalies manually.

This exercise includes the following tasks:

* Triggering anomalies using IoT Simulator App
* Detect Anomalies using built-in Machine Learning Model

### Task 1: Triggering anomalies using IoT Simulator App

1. On the Desktop of the provide machine on the left, open IoT simulator app.

1. From the IoT simulator app dialog , Click on **Anomaly** button next to stop telemetry button that sends anomalies to IoT hub.

    >**Note**: If the devices are not sending telemetry , make sure to start IoT Simulator App and simulate the devices by sending telemetry.
    
1. On the grid, list of telemetry messages that are transmitted are displayed.Observe the simulated temperature value.


### Task 2: Detect Anomalies using built-in Machine Learning Model

In this task you will be using the following ML Model:

**Built-in Machine Learning Model** - The AnomalyDetection_SpikeAndDip function uses a sliding window to analyze data for anomalies. The sliding window could be, for example, the most recent two minutes of telemetry data. The window advances in near real-time with the flow of telemetry. If the size of the sliding window is increased to include more data, the accuracy of anomaly detection will increase as well (however, the latency also increases, so a balance must be found).

1. On the resource group tile, click **iot-{deployment-id}** and select the stream analytics job named **iot-streamjob-{deployment-id}**.

1. On the left-side menu under **Job topology**, click on **Query**.

1. Copy the following SQL query, and then replace it with the existing query.

    ```sql
     WITH AnomalyDetectionStep AS
     (
       SELECT
           EventProcessedUtcTime AS time,
           CAST(temp AS float) AS temp,
           AnomalyDetection_SpikeAndDip(CAST(temp AS float), 90, 120, 'spikesanddips')
               OVER(LIMIT DURATION(second, 120)) AS SpikeAndDipScores
       FROM iothubinput
    )
       SELECT
           time,
           temp,
           CAST(GetRecordPropertyValue(SpikeAndDipScores, 'Score') AS float) AS
           SpikeAndDipScore,
           CAST(GetRecordPropertyValue(SpikeAndDipScores, 'IsAnomaly') AS bigint) AS
           IsSpikeAndDipAnomaly
      INTO powerbioutput
      FROM AnomalyDetectionStep
    ```

    > **Note**:  This first section of this query takes the temperature data, and examines the previous 120 seconds worth. The `AnomalyDetection_SpikeAndDip` function will return a `Score` parameter, and an `IsAnomaly` parameter. The score is how certain the ML model is that the given value is an anomaly, specified through a percentage value. If the score exceeds 90%, the `IsAnomaly` parameter has a value of 1, otherwise `IsAnomaly` has a value of 0. Notice the 120 and 90 parameters in the first section of the query. The second section of the query sends the time, temperature, and anomaly parameters to `powerbioutput`.

1. Verify that the query editor now lists 1 Input and 3 Outputs:

    * `Inputs`
      * `iothubinput`
    * `Outputs`
      * `bloboutput`
      * `powerbioutput`     
      * `servicebusoutput`

    If you see more than 1 of each then you likely have a typo in your query or in the name you used for the input or output - correct the issue before moving on.

1. To save the query, click **Save query**.

1. On the left-side menu, click **Overview**.

1. Near the top of the blade, click **Start**.

   > **Note**: If your stream job fails to start, then perform the following steps:
     *  Select **Outputs** under Job Topology and select **powerbioutput**
     * Then, in the blade that comes up, click on Renew authorization and when prompted for Azure Credentials, provide the Azure Username and Password from the environment details tab and then click on Save.

1. On the **Start job** pane, under **Job output start time**, ensure **Now** is selected, and then click **Start**.

In order for a human operator to easily interpret the output from this query, you need to visualize the data in a friendly way. One way of doing this visualization is to create a Power BI dashboard. You will be doing that in the next exercise.
